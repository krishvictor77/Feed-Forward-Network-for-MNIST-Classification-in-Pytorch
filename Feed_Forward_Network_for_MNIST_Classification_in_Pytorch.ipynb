{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feed Forward Network for MNIST Classification in Pytorch",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MrT7JxhPLlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing required libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as dsets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flj9B2L5PxI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 1: LOADING DATASET\n",
        "\n",
        "train_dataset = dsets.MNIST(root='./data', \n",
        "                            train=True, \n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)\n",
        "\n",
        "test_dataset = dsets.MNIST(root='./data', \n",
        "                           train=False, \n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtTaelIKPxPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 2: MAKING DATASET ITERABLE\n",
        "\n",
        "batch_size = 100\n",
        "n_iters = 6000  \n",
        "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
        "num_epochs = int(num_epochs)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKQXEfFdcfsy",
        "colab_type": "code",
        "outputId": "c336d9d9-adcc-409c-ea2c-9a4448a0f7b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_dataset)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BT3bpNbRckSY",
        "colab_type": "code",
        "outputId": "885686ad-9112-463e-cca7-4bbebc03e6c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "num_epochs"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZKxtSBqPxNI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 3: CREATE MODEL CLASS\n",
        "\n",
        "class FeedforwardNeuralNetModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim):\n",
        "        super(FeedforwardNeuralNetModel, self).__init__()\n",
        "        \n",
        "        # Linear function 1: 784 --> 100\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1) \n",
        "        # Non-linearity 1\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Linear function 2: 100 --> 100\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        # Non-linearity 2\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Linear function 3: 100 --> 100\n",
        "        self.fc3 = nn.Linear(hidden_dim2, hidden_dim3)\n",
        "        # Non-linearity 3\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "        # Linear function 4 (readout): 100 --> 10\n",
        "        self.fc4 = nn.Linear(hidden_dim3, output_dim)  \n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        out = self.fc1(x)\n",
        "        # Non-linearity 1\n",
        "        out = self.sigmoid(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc2(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Linear function 2\n",
        "        out = self.fc3(out)\n",
        "        # Non-linearity 2\n",
        "        out = self.tanh(out)\n",
        "\n",
        "        # Linear function 4 (readout)\n",
        "        out = self.fc4(out)\n",
        "        return out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1vS0ozmPxHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 4: INSTANTIATE MODEL CLASS\n",
        "\n",
        "input_dim = 784\n",
        "hidden_dim1 = 256\n",
        "hidden_dim2 = 128\n",
        "hidden_dim3 = 64\n",
        "output_dim = 10\n",
        "\n",
        "model = FeedforwardNeuralNetModel(input_dim, hidden_dim1, hidden_dim2, hidden_dim3, output_dim)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2fpYmWPPxFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 5: INSTANTIATE LOSS CLASS\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxONoSl3PxCq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
        "\n",
        "learning_rate = 0.1\n",
        "momentum=0.9\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=momentum)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34Fo3RYPw_w",
        "colab_type": "code",
        "outputId": "624f9bf9-a252-4c6f-807b-23503cf0c6b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#STEP 7: TRAIN THE MODEL\n",
        "\n",
        "iter = 0\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # Load images with gradient accumulation capabilities\n",
        "        images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "        # Clear gradients w.r.t. parameters\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass to get output/logits\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Calculate Loss: softmax --> cross entropy loss\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Getting gradients w.r.t. parameters\n",
        "        loss.backward()\n",
        "\n",
        "        # Updating parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        iter += 1\n",
        "\n",
        "        if iter % 500 == 0:\n",
        "            # Calculate Accuracy         \n",
        "            correct = 0\n",
        "            total = 0\n",
        "            # Iterate through test dataset\n",
        "            for images, labels in test_loader:\n",
        "                # Load images with gradient accumulation capabilities\n",
        "                images = images.view(-1, 28*28).requires_grad_()\n",
        "\n",
        "                # Forward pass only to get logits/output\n",
        "                outputs = model(images)\n",
        "\n",
        "                # Get predictions from the maximum value\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "                # Total number of labels\n",
        "                total += labels.size(0)\n",
        "\n",
        "                # Total correct predictions\n",
        "                correct += (predicted == labels).sum()\n",
        "\n",
        "            accuracy = float(100 * correct / total)\n",
        "\n",
        "            # Print Loss for each Epoch \n",
        "            print('Iteration: {}. Loss: {}. Accuracy: {}'.format(iter, loss.item(), accuracy))\n",
        "            \n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration: 500. Loss: 0.35782524943351746. Accuracy: 91.0\n",
            "Iteration: 1000. Loss: 0.19663062691688538. Accuracy: 94.0\n",
            "Iteration: 1500. Loss: 0.1515016257762909. Accuracy: 95.0\n",
            "Iteration: 2000. Loss: 0.0488416850566864. Accuracy: 96.0\n",
            "Iteration: 2500. Loss: 0.12553375959396362. Accuracy: 96.0\n",
            "Iteration: 3000. Loss: 0.058085847645998. Accuracy: 97.0\n",
            "Iteration: 3500. Loss: 0.05741754546761513. Accuracy: 97.0\n",
            "Iteration: 4000. Loss: 0.02063162811100483. Accuracy: 97.0\n",
            "Iteration: 4500. Loss: 0.0595187172293663. Accuracy: 97.0\n",
            "Iteration: 5000. Loss: 0.007015925832092762. Accuracy: 97.0\n",
            "Iteration: 5500. Loss: 0.019600030034780502. Accuracy: 97.0\n",
            "Iteration: 6000. Loss: 0.06613089889287949. Accuracy: 97.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}